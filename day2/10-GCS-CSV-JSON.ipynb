{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79219a9a-948d-4a34-aa65-6e3a5f69bc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-02T03:17:32.782359Z main ERROR Reconfiguration failed: No configuration found for '63e31ee' at 'null' in 'null'\n",
      "2025-12-02T03:17:33.888567Z Thread-3 ERROR Reconfiguration failed: No configuration found for '10be4640' at 'null' in 'null'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_config.TableConfig at 0x7f13a35f7e50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "import get_env\n",
    "env = get_env.get_remote_env()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "t_env.get_config().set(\"parallelism.default\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54ecf2ba-b03c-43b9-b6ed-9f87cad37a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.configuration.Configuration at 0x7f13a2744640>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conf = t_env.get_config().get_configuration()\n",
    "conf.set_string(\"fs.allowed-fallback-filesystems\", \"hadoop\")\n",
    "conf.set_string(\"fs.gs.project.id\", \"flink-demo-470113\")\n",
    "conf.set_string(\"fs.gs.auth.service.account.json.keyfile\", \"/etc/gcp/key.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d21a27-04b2-46df-a762-ca49b622c2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema ===\n",
      "(\n",
      "  `movieId` INT,\n",
      "  `title` STRING,\n",
      "  `genres` STRING\n",
      ")\n",
      "=== Data ===\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| op |     movieId |                          title |                         genres |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| +I |      <NULL> |                          title |                         genres |\n",
      "| +I |           1 |               Toy Story (1995) | Adventure|Animation|Childre... |\n",
      "| +I |           2 |                 Jumanji (1995) |     Adventure|Children|Fantasy |\n",
      "| +I |           3 |        Grumpier Old Men (1995) |                 Comedy|Romance |\n",
      "| +I |           4 |       Waiting to Exhale (1995) |           Comedy|Drama|Romance |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Optional: set your project id (helps GCS connector)\n",
    "# t_env.get_config().get_configuration().set_string(\n",
    "#     \"fs.gs.project.id\", \"your-gcp-project-id\"\n",
    "# )\n",
    "\n",
    "# 1) Drop if exists\n",
    "t_env.execute_sql(\"DROP TABLE IF EXISTS movies\")\n",
    "\n",
    "# 2) Register the CSV table on GCS\n",
    "# NOTE: csv.ignore-parse-errors='true' will skip the header row\n",
    "ddl = \"\"\"\n",
    "CREATE TABLE movies (\n",
    "    movieId INT,\n",
    "    title   STRING,\n",
    "    genres  STRING\n",
    ") WITH (\n",
    "    'connector' = 'filesystem',\n",
    "    'path'      = 'gs://gk2-datalake/bronze/movies/',\n",
    "    'format'    = 'csv',\n",
    "    'csv.ignore-parse-errors' = 'true',\n",
    "    'csv.source.ignore-first-line' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "t_env.execute_sql(ddl)\n",
    "\n",
    "# 3) Get the table and print schema\n",
    "movies = t_env.from_path(\"movies\")\n",
    "print(\"=== Schema ===\")\n",
    "movies.print_schema()\n",
    "\n",
    "# 4) Print the data (bounded filesystem source -> will finish)\n",
    "print(\"=== Data ===\")\n",
    "#result = movies.execute()\n",
    "#result.print()\n",
    "\n",
    "limited = movies.limit(5)\n",
    "limited.execute().print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea6e36e2-8360-43d4-8912-14f2cc5e5899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7f139efb5ea0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"DROP TABLE IF EXISTS movies_json\")\n",
    "# Sink table: filesystem JSON\n",
    "ddl_sink = \"\"\"\n",
    "CREATE TABLE movies_json (\n",
    "    movieId INT,\n",
    "    title   STRING,\n",
    "    genres  STRING\n",
    ") WITH (\n",
    "    'connector' = 'filesystem',\n",
    "    'path'      = 'gs://gk2-datalake/bronze/movies-json/',\n",
    "    'format'    = 'json'\n",
    ")\n",
    "\"\"\"\n",
    "t_env.execute_sql(ddl_sink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20dd22aa-d010-420d-9a1d-dd8a1e3364af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job to write JSON to GCS...\n",
      "Submitted. Job info: <pyflink.table.table_result.TableResult object at 0x7f13a35f4ee0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Insert while filtering out rows where movieId is NULL (this removes the header)\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO movies_json\n",
    "SELECT movieId, title, genres\n",
    "FROM movies\n",
    "WHERE movieId IS NOT NULL\n",
    "\"\"\"\n",
    "# execute the insert (returns a TableResult)\n",
    "print(\"Starting job to write JSON to GCS...\")\n",
    "job_result = t_env.execute_sql(insert_sql)\n",
    "print(\"Submitted. Job info:\", job_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d41e72-873c-428f-8f0c-a67ce8554c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema ===\n",
      "=== Data ===\n",
      "(\n",
      "  `movieId` INT,\n",
      "  `title` STRING,\n",
      "  `genres` STRING\n",
      ")\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| op |     movieId |                          title |                         genres |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| +I |           1 |               Toy Story (1995) | Adventure|Animation|Childre... |\n",
      "| +I |           2 |                 Jumanji (1995) |     Adventure|Children|Fantasy |\n",
      "| +I |           3 |        Grumpier Old Men (1995) |                 Comedy|Romance |\n",
      "| +I |           4 |       Waiting to Exhale (1995) |           Comedy|Drama|Romance |\n",
      "| +I |           5 | Father of the Bride Part II... |                         Comedy |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "# Drop temporary table safely\n",
    "try:\n",
    "    t_env.execute_sql(\"DROP TEMPORARY TABLE movies_json\")\n",
    "except Exception:\n",
    "    pass   # ignore error if it doesn't exist\n",
    "    \n",
    "# Register the JSON table\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE movies_json (\n",
    "    movieId INT,\n",
    "    title STRING,\n",
    "    genres STRING\n",
    ") WITH (\n",
    "    'connector' = 'filesystem',\n",
    "    'path' = 'gs://gk2-datalake/bronze/movies-json/',\n",
    "    'format' = 'json',\n",
    "    'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 3) Get the table and print schema\n",
    "movies = t_env.from_path(\"movies_json\")\n",
    "print(\"=== Schema ===\")\n",
    "movies.print_schema()\n",
    "\n",
    "# 4) Print the data (bounded filesystem source -> will finish)\n",
    "print(\"=== Data ===\")\n",
    "#result = movies.execute()\n",
    "#result.print()\n",
    "\n",
    "limited = movies.limit(5)\n",
    "limited.execute().print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72c907de-b1ca-45ce-b135-06545343c5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| op |     movieId |                          title |                         genres |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| +I |           1 |               Toy Story (1995) | Adventure|Animation|Childre... |\n",
      "| +I |           2 |                 Jumanji (1995) |     Adventure|Children|Fantasy |\n",
      "| +I |           3 |        Grumpier Old Men (1995) |                 Comedy|Romance |\n",
      "| +I |           4 |       Waiting to Exhale (1995) |           Comedy|Drama|Romance |\n",
      "| +I |           5 | Father of the Bride Part II... |                         Comedy |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "movies_sql = t_env.sql_query(\"SELECT * FROM movies_json LIMIT 5\")\n",
    "movies_sql.execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8daca4cf-77bb-4665-9922-4a5cd6f2b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(1, 'Toy Story (1995)', 'Adventure|Animation|Children|Comedy|Fantasy')>\n",
      "<Row(2, 'Jumanji (1995)', 'Adventure|Children|Fantasy')>\n",
      "<Row(3, 'Grumpier Old Men (1995)', 'Comedy|Romance')>\n",
      "<Row(4, 'Waiting to Exhale (1995)', 'Comedy|Drama|Romance')>\n",
      "<Row(5, 'Father of the Bride Part II (1995)', 'Comedy')>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query 5 records\n",
    "# Returns TableResult\n",
    "result = t_env.execute_sql(\"SELECT * FROM movies_json LIMIT 5\")\n",
    "\n",
    "print \n",
    "it = result.collect()\n",
    "for i, row in enumerate(it):\n",
    "    print(row)            # prints Row(movieId=..., title='...', genres='...')\n",
    "\n",
    "it.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "225f75a2-c86d-4581-ace2-cc2548557ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| op |     movieId |                    title_upper |                         genres |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "| +I |           1 |               TOY STORY (1995) | Adventure|Animation|Childre... |\n",
      "| +I |           3 |        GRUMPIER OLD MEN (1995) |                 Comedy|Romance |\n",
      "| +I |           4 |       WAITING TO EXHALE (1995) |           Comedy|Drama|Romance |\n",
      "| +I |           5 | FATHER OF THE BRIDE PART II... |                         Comedy |\n",
      "| +I |           7 |                 SABRINA (1995) |                 Comedy|Romance |\n",
      "+----+-------------+--------------------------------+--------------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "# upper case, where condition\n",
    "\n",
    "\n",
    "movies_sql = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        movieId,\n",
    "        UPPER(title) AS title_upper,\n",
    "        genres\n",
    "    FROM movies_json\n",
    "    WHERE genres LIKE '%Comedy%'\n",
    "    LIMIT 5 \n",
    "\"\"\")\n",
    "\n",
    "movies_sql.execute().print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255b797d-4038-4e38-be2a-95006e0c8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Drop tables if they exist (safe pattern)\n",
    "for q in [\n",
    "    \"DROP TEMPORARY TABLE IF EXISTS ratings_csv\",    # note: IF EXISTS may error for TEMP in some versions; try/except below\n",
    "    \"DROP TEMPORARY TABLE IF EXISTS ratings_json\"\n",
    "]:\n",
    "    try:\n",
    "        t_env.execute_sql(q)\n",
    "    except Exception:\n",
    "        # ignore - some Flink builds disallow IF EXISTS for temporary tables\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0fee0b1-e402-48e0-8093-13adbd77e392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema ===\n",
      "=== Data ===\n",
      "(\n",
      "  `userId` INT,\n",
      "  `movieId` INT,\n",
      "  `rating` DOUBLE,\n",
      "  `timestamp` BIGINT\n",
      ")\n",
      "+----+-------------+-------------+--------------------------------+----------------------+\n",
      "| op |      userId |     movieId |                         rating |            timestamp |\n",
      "+----+-------------+-------------+--------------------------------+----------------------+\n",
      "| +I |      <NULL> |      <NULL> |                         <NULL> |               <NULL> |\n",
      "| +I |           1 |           1 |                            4.0 |            964982703 |\n",
      "| +I |           1 |           3 |                            4.0 |            964981247 |\n",
      "| +I |           1 |           6 |                            4.0 |            964982224 |\n",
      "| +I |           1 |          47 |                            5.0 |            964983815 |\n",
      "+----+-------------+-------------+--------------------------------+----------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE ratings_csv (\n",
    "  `userId` INT,\n",
    "  `movieId` INT,\n",
    "  `rating` DOUBLE,\n",
    "  `timestamp` BIGINT\n",
    ") WITH (\n",
    "  'connector' = 'filesystem',\n",
    "  'path' = 'gs://gk2-datalake/bronze/ratings/',\n",
    "  'format' = 'csv',\n",
    "  'csv.field-delimiter' = ',',\n",
    "  'csv.ignore-parse-errors' = 'true',\n",
    "  'csv.allow-comments' = 'false',\n",
    "  'csv.first-line-as-header' = 'true'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 3) Get the table and print schema\n",
    "ratings_csv = t_env.from_path(\"ratings_csv\")\n",
    "print(\"=== Schema ===\")\n",
    "ratings_csv.print_schema()\n",
    "\n",
    "# 4) Print the data (bounded filesystem source -> will finish)\n",
    "print(\"=== Data ===\")\n",
    "\n",
    "limited = ratings_csv.limit(5)\n",
    "limited.execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22862c5b-8403-43a3-9ef6-7a24aa516726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7f13a35f7f40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Create JSON sink table (newline-delimited JSON)\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE ratings_json (\n",
    "  `userId` INT,\n",
    "  `movieId` INT,\n",
    "  `rating` DOUBLE,\n",
    "  `timestamp` BIGINT\n",
    ") WITH (\n",
    "  'connector' = 'filesystem',\n",
    "  'path' = 'gs://gk2-datalake/bronze/ratings-json/',\n",
    "  'format' = 'json',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e4e5f64-8649-44a7-8d0b-943aaf383326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| op |     movieId |                         rating |               rate_date |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| +I |           1 |                            4.0 | 2000-07-30 18:45:03.000 |\n",
      "| +I |           3 |                            4.0 | 2000-07-30 18:20:47.000 |\n",
      "| +I |           6 |                            4.0 | 2000-07-30 18:37:04.000 |\n",
      "| +I |          47 |                            5.0 | 2000-07-30 19:03:35.000 |\n",
      "| +I |          50 |                            5.0 | 2000-07-30 18:48:51.000 |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "from pyflink.table.expressions import col, to_timestamp_ltz, lit\n",
    "from pyflink.table import DataTypes\n",
    "\n",
    "ratings = t_env.from_path(\"ratings_csv\")\n",
    "\n",
    "transformed = (\n",
    "    ratings\n",
    "      # <-- IMPORTANT: do NOT call is_not_null(), use the Expression object\n",
    "      .filter(col(\"userId\").is_not_null)\n",
    "\n",
    "      .add_columns(\n",
    "          to_timestamp_ltz(\n",
    "              # cast to STRING first, trim whitespace, then cast to BIGINT, multiply by 1000\n",
    "              (col(\"timestamp\").cast(DataTypes.STRING()).trim().cast(DataTypes.BIGINT()) * lit(1000)),\n",
    "              3\n",
    "          ).alias(\"rate_date\")\n",
    "      )\n",
    "\n",
    "      # .drop_columns(col(\"timestamp\"))  \n",
    "\n",
    "      # explicitly select columns to exclude userId\n",
    "      .select(\n",
    "          col(\"movieId\"),\n",
    "          col(\"rating\"),\n",
    "          # col(\"timestamp\"),\n",
    "          col(\"rate_date\")   # alias created above\n",
    "      )\n",
    ")\n",
    "\n",
    "transformed.limit(5).execute().print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c37b1b-5dd9-4869-91a1-6af10e8881a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| op |     movieId |                         rating |               rate_date |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| +I |          31 |                            0.5 | 2011-05-27 02:32:58.000 |\n",
      "| +I |         527 |                            0.5 | 2011-05-27 02:44:35.000 |\n",
      "| +I |         647 |                            0.5 | 2011-05-27 02:33:39.000 |\n",
      "| +I |         688 |                            0.5 | 2011-05-27 02:43:48.000 |\n",
      "| +I |         720 |                            0.5 | 2011-05-27 02:33:15.000 |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "filtered = transformed.filter(col(\"rating\") == 0.5)\n",
    "filtered.limit(5).execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ff5b32-be68-4e67-b190-df21794cf011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View, View is not materilized, temp table is materialized\n",
    "try:\n",
    "    t_env.execute_sql(\"DROP TEMPORARY VIEW transformed_view\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    t_env.execute_sql(\"DROP TEMPORARY VIEW filtered_view\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad0caa9d-5b7d-418a-9490-81a7d5520182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| op |     movieId |                         rating |               rate_date |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "| +I |           1 |                            4.0 | 2000-07-30 18:45:03.000 |\n",
      "| +I |           3 |                            4.0 | 2000-07-30 18:20:47.000 |\n",
      "| +I |           6 |                            4.0 | 2000-07-30 18:37:04.000 |\n",
      "| +I |          47 |                            5.0 | 2000-07-30 19:03:35.000 |\n",
      "| +I |          50 |                            5.0 | 2000-07-30 18:48:51.000 |\n",
      "+----+-------------+--------------------------------+-------------------------+\n",
      "5 rows in set\n"
     ]
    }
   ],
   "source": [
    "# safe drop (ignore errors if not present)\n",
    "for v in (\"transformed_view\", \"filtered_view\"):\n",
    "    try:\n",
    "        t_env.execute_sql(f\"DROP TEMPORARY VIEW {v}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# create transformed_view using quoted identifier for the timestamp column\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY VIEW transformed_view AS\n",
    "SELECT\n",
    "  movieId,\n",
    "  rating,\n",
    "  TO_TIMESTAMP_LTZ(\n",
    "      CAST(TRIM(CAST(`timestamp` AS STRING)) AS BIGINT) * 1000,\n",
    "      3\n",
    "  ) AS rate_date\n",
    "FROM ratings_csv\n",
    "WHERE `userId` IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# preview 5 rows\n",
    "t_env.sql_query(\"SELECT * FROM transformed_view LIMIT 5\").execute().print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab7139ca-b2d7-4fee-bfac-d73f187f6304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7f139efb7760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Safe create + insert into partitioned JSON sink (copy-paste)\n",
    "from pyflink.table import TableResult\n",
    "\n",
    "# 1) safe drop if exists\n",
    "try:\n",
    "    t_env.execute_sql(\"DROP TABLE IF EXISTS movies_partitioned_json\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# use backtick ` for reserved column name \n",
    "\n",
    "# 2) create partitioned table (quote potentially reserved names with backticks)\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TABLE movies_partitioned_json (\n",
    "  movieId INT,\n",
    "  rating DOUBLE,\n",
    "  rate_date TIMESTAMP(3),\n",
    "  `year` INT,\n",
    "  `month` INT,\n",
    "  `day` INT\n",
    ") PARTITIONED BY (`year`, `month`, `day`)\n",
    "WITH (\n",
    "  'connector' = 'filesystem',\n",
    "  'path' = 'gs://gk2-datalake/bronze/ratings-json-partition/',\n",
    "  'format' = 'json',\n",
    "  'json.ignore-parse-errors' = 'true',\n",
    "  -- optional partition commit (may be unsupported on some builds; remove if parser complains)\n",
    "  'sink.partition-commit.policy.kind' = 'success-file',\n",
    "  'sink.partition-commit.delay' = '0s',\n",
    "  'sink.partition-commit.trigger' = 'partition-time'\n",
    ")\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce3260a7-f849-4f9c-b4ad-5fece352e474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting INSERT job (with casts)...\n",
      "Insert submitted: <pyflink.table.table_result.TableResult object at 0x7f139efb5a50>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# cast output types to match sink schema: TIMESTAMP(3) and INT for partitions\n",
    "insert_stmt = \"\"\"\n",
    "INSERT INTO movies_partitioned_json\n",
    "SELECT\n",
    "  movieId,\n",
    "  rating,\n",
    "  CAST(rate_date AS TIMESTAMP(3))                      AS rate_date,\n",
    "  CAST(EXTRACT(YEAR FROM rate_date)  AS INT)           AS `year`,\n",
    "  CAST(EXTRACT(MONTH FROM rate_date) AS INT)           AS `month`,\n",
    "  CAST(EXTRACT(DAY FROM rate_date)   AS INT)           AS `day`\n",
    "FROM transformed_view\n",
    "\"\"\"\n",
    "print(\"Submitting INSERT job (with casts)...\")\n",
    "insert_result = t_env.execute_sql(insert_stmt)\n",
    "print(\"Insert submitted:\", insert_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ca738df-74bf-4596-a016-f156de18529f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job 0f5facf172ea17017e22eaf948f24f5e\n",
      "status: JobStatus.RUNNING JobStatus.RUNNING\n"
     ]
    }
   ],
   "source": [
    "job_id = insert_result.get_job_client().get_job_id()\n",
    "print (\"job\", job_id)\n",
    "\n",
    "status = insert_result.get_job_client().get_job_status().result()\n",
    " \n",
    "# keep checking until FINISHED\n",
    "print(\"status:\", status, str(status))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "930a6052-41bc-47bc-8666-0b338c0cb081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verify query error (ok if job still running): org.apache.flink.table.api.TableException: Fetch partitions fail.\n",
      "\tat org.apache.flink.connector.file.table.FileSystemTableSource.listPartitions(FileSystemTableSource.java:346)\n",
      "\tat org.apache.flink.connector.file.table.FileSystemTableSource.getOrFetchPartitions(FileSystemTableSource.java:437)\n",
      "\tat org.apache.flink.connector.file.table.FileSystemTableSource.getScanRuntimeProvider(FileSystemTableSource.java:130)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.common.CommonExecTableSourceScan.translateToPlanInternal(CommonExecTableSourceScan.java:121)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:259)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecExchange.translateToPlanInternal(StreamExecExchange.java:99)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:259)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecRank.translateToPlanInternal(StreamExecRank.java:205)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecLimit.translateToPlanInternal(StreamExecLimit.java:127)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecEdge.translateToPlan(ExecEdge.java:259)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.stream.StreamExecSink.translateToPlanInternal(StreamExecSink.java:177)\n",
      "\tat org.apache.flink.table.planner.plan.nodes.exec.ExecNodeBase.translateToPlan(ExecNodeBase.java:168)\n",
      "\tat org.apache.flink.table.planner.delegation.StreamPlanner.$anonfun$translateToPlan$1(StreamPlanner.scala:85)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n",
      "\tat org.apache.flink.table.planner.delegation.StreamPlanner.translateToPlan(StreamPlanner.scala:84)\n",
      "\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:180)\n",
      "\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1308)\n",
      "\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1133)\n",
      "\tat org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:477)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.base/java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'gs'. The scheme is directly supported by Flink through the following plugin(s): flink-gs-fs-hadoop. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/.\n",
      "\tat org.apache.flink.core.fs.FileSystem.getUnguardedFileSystem(FileSystem.java:514)\n",
      "\tat org.apache.flink.core.fs.FileSystem.get(FileSystem.java:408)\n",
      "\tat org.apache.flink.core.fs.Path.getFileSystem(Path.java:284)\n",
      "\tat org.apache.flink.connector.file.table.FileSystemTableSource.listPartitions(FileSystemTableSource.java:329)\n",
      "\t... 41 more\n",
      "\n",
      "Check GCS path: gs://gk2-datalake/bronze/ratings-json-partition/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) quick verify (may return results only after job finishes and files are visible)\n",
    "try:\n",
    "    t_env.sql_query(\"SELECT movieId, rating, rate_date, `year`, `month`, `day` FROM movies_partitioned_json LIMIT 5\").execute().print()\n",
    "except Exception as e:\n",
    "    print(\"Verify query error (ok if job still running):\", e)\n",
    "    print(\"Check GCS path: gs://gk2-datalake/bronze/ratings-json-partition/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdde5a9-cc35-405a-ac8d-79429915fad5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
