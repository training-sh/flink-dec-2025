{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd063a68-c668-438e-ac00-60038bf02c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-04T20:59:06.036316Z main ERROR Reconfiguration failed: No configuration found for '567d299b' at 'null' in 'null'\n",
      "2025-12-04T20:59:07.160898Z Thread-3 ERROR Reconfiguration failed: No configuration found for '7b8574d6' at 'null' in 'null'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.datastream.stream_execution_environment.StreamExecutionEnvironment at 0x7fbf38dd1ae0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment\n",
    "\n",
    "import get_env\n",
    "env = get_env.get_remote_env()\n",
    "t_env = StreamTableEnvironment.create(env)\n",
    "t_env.get_config().set(\"parallelism.default\", \"1\")\n",
    "\n",
    "conf = t_env.get_config().get_configuration()\n",
    "conf.set_string(\"fs.allowed-fallback-filesystems\", \"hadoop\")\n",
    "conf.set_string(\"fs.gs.project.id\", \"flink-demo-470113\")\n",
    "conf.set_string(\"fs.gs.auth.service.account.json.keyfile\", \"/etc/gcp/key.json\")\n",
    "\n",
    "# ICEBERD NEED CHECKPOINTING for write\n",
    "env.enable_checkpointing(10000)   # time in ms\n",
    "# or \n",
    "#conf = t_env.get_config().get_configuration()\n",
    "#conf.set_string(\"execution.checkpointing.interval\", \"10 s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d7f469-08f1-4eee-958a-e933d606c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1|{\"customer_id\":1,\"full_name\":\"Vel\",\"email\":\"vel@example.com\",\"signup_date\":\"2025-01-10\",\"country\":\"India\",\"city\":\"Chennai\",\"segment\":\"Prime\"}\n",
    "#  docker logs -f flink-taskmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fd7729-d8e5-43f1-b113-b97291e3677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATALAKE_WAREHOUSE = 'gs://gks-datalake/iceberg-warehouse/'\n",
    "BOOTSTRAP = \"broker:9092\"    # change if needed\n",
    "GROUP_ID_PREFIX  = \"ecomm-flink-tr-12\"\n",
    "TOPIC_PREFIX = \"gks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4533cc8-fd55-4aeb-a350-79b2f53fc71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd2e60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop old catalog if you were using it\n",
    "\n",
    "# t_env.execute_sql(\"DROP CATALOG IF EXISTS ecomm\")\n",
    "\n",
    "# Create new Iceberg catalog 'ecomm'\n",
    "t_env.execute_sql(f\"\"\"\n",
    "CREATE CATALOG IF NOT EXISTS ecomm WITH (\n",
    "  'type' = 'iceberg',\n",
    "  'catalog-type' = 'hadoop',\n",
    "  -- GCS path for metadata/warehouse; bucket must exist and be writable\n",
    "  'warehouse' = '{DATALAKE_WAREHOUSE}',\n",
    "  'property-version' = '1'\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6329f7bb-e7c6-408d-bb6a-542a0c1c4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dim_customers_kafka…\n",
      "Tables now: ['dim_customers_kafka']\n",
      "Creating dim_customers_print…\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd3640>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0) Clean up\n",
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS dim_customers_kafka\")\n",
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS dim_customers_print\")\n",
    "\n",
    "# 1) Kafka source table\n",
    "BOOTSTRAP = \"broker:9092\"  # adjust if needed\n",
    "\n",
    "DIM_CUSTOMERS_KAFKA_DDL = f\"\"\"\n",
    "CREATE TEMPORARY TABLE dim_customers_kafka (\n",
    "  customer_id INT,\n",
    "  full_name   STRING,\n",
    "  email       STRING,\n",
    "  signup_date STRING,\n",
    "  country     STRING,\n",
    "  city        STRING,\n",
    "  segment     STRING\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'dim_customers',\n",
    "  'properties.bootstrap.servers' = '{BOOTSTRAP}',\n",
    "  'properties.group.id' = 'test-dim-customers',\n",
    "  'scan.startup.mode' = 'earliest-offset',\n",
    "  'format' = 'json',\n",
    "  'json.fail-on-missing-field' = 'false',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating dim_customers_kafka…\")\n",
    "t_env.execute_sql(DIM_CUSTOMERS_KAFKA_DDL)\n",
    "\n",
    "# (optional) verify it is actually registered\n",
    "print(\"Tables now:\", t_env.list_tables())\n",
    "\n",
    "# 2) PRINT sink\n",
    "DIM_CUSTOMERS_PRINT_DDL = \"\"\"\n",
    "CREATE TEMPORARY TABLE dim_customers_print (\n",
    "  customer_id INT,\n",
    "  full_name   STRING,\n",
    "  email       STRING,\n",
    "  signup_date STRING,\n",
    "  country     STRING,\n",
    "  city        STRING,\n",
    "  segment     STRING\n",
    ") WITH (\n",
    "  'connector' = 'print'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "print(\"Creating dim_customers_print…\")\n",
    "t_env.execute_sql(DIM_CUSTOMERS_PRINT_DDL)\n",
    "\n",
    "# 3) Insert from Kafka -> print sink  (this starts a job)\n",
    "# print(\"Starting INSERT job…\")\n",
    "# insert_result = t_env.execute_sql(\"\"\"\n",
    "# INSERT INTO dim_customers_print\n",
    "# SELECT\n",
    "#   customer_id,\n",
    "#   full_name,\n",
    "#   email,\n",
    "#   signup_date,\n",
    "#   country,\n",
    "#   city,\n",
    "#   segment\n",
    "# FROM dim_customers_kafka\n",
    "# \"\"\")\n",
    "\n",
    "# print(\"Job submitted:\", insert_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a89c0e7-08c8-4076-bbb4-b645d6eafdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.sql_query(\"SELECT * FROM dim_customers_kafka LIMIT 1 \").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56913a94-8367-4f48-a8e9-a6bff3a78d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_PRODUCTS_KAFKA_DDL = \"\"\"\n",
    "CREATE TEMPORARY TABLE dim_products_kafka (\n",
    "  product_id   INT,\n",
    "  product_name STRING,\n",
    "  category     STRING,\n",
    "  subcategory  STRING,\n",
    "  brand        STRING,\n",
    "  unit_price   DECIMAL(10,2),\n",
    "  active       BOOLEAN\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'dim_products',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    "  'properties.group.id' = 'ecomm-flink-dim-products',\n",
    "  'scan.startup.mode' = 'earliest-offset',\n",
    "  'format' = 'json',\n",
    "  'json.fail-on-missing-field' = 'false',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS dim_products_kafka;\").wait()\n",
    "\n",
    "t_env.execute_sql(DIM_PRODUCTS_KAFKA_DDL).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12839196-ca8c-47d3-9b39-da1f4fb308e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_env.sql_query(\"SELECT * FROM dim_products_kafka LIMIT 1 \").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b8b78e8-ce5b-4e58-b26e-2242450482be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will discuss water mark on Tuesday\n",
    "# WATERMARK FOR order_datetime AS order_datetime - INTERVAL '5' SECOND\n",
    "FACT_ORDERS_KAFKA_DDL = \"\"\"\n",
    "CREATE TEMPORARY TABLE fact_orders_kafka (\n",
    "  order_id       INT,\n",
    "  customer_id    INT,\n",
    "  order_datetime TIMESTAMP(3),\n",
    "  order_status   STRING,\n",
    "  payment_method STRING,\n",
    "  shipping_fee   DECIMAL(10,2),\n",
    "  discount_amt   DECIMAL(10,2)\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'fact_orders',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    "  'properties.group.id' = 'ecomm-flink-fact-orders',\n",
    "  'scan.startup.mode' = 'earliest-offset',\n",
    "  'format' = 'json',\n",
    "   'json.timestamp-format.standard' = 'ISO-8601',\n",
    "  'json.fail-on-missing-field' = 'false',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS fact_orders_kafka;\").wait()\n",
    "\n",
    "t_env.execute_sql(FACT_ORDERS_KAFKA_DDL).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98548077-2f79-4409-b566-f2b10c57c215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.sql_query(\"SELECT * FROM fact_orders_kafka LIMIT 1 \").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bddd8944-aebd-4db1-85d7-3c187fa60cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "FACT_ORDERS_ITEMS_KAFKA_DDL = \"\"\"\n",
    "CREATE TEMPORARY TABLE fact_order_items_kafka (\n",
    "  order_item_id INT,\n",
    "  order_id      INT,\n",
    "  product_id    INT,\n",
    "  quantity      INT,\n",
    "  item_price    DECIMAL(10,2)\n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'fact_order_items',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    "  'properties.group.id' = 'ecomm-flink-fact-order-items',\n",
    "  'scan.startup.mode' = 'earliest-offset',\n",
    "  'format' = 'json',\n",
    "  'json.timestamp-format.standard' = 'ISO-8601',\n",
    "  'json.fail-on-missing-field' = 'false',\n",
    "  'json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS fact_order_items_kafka;\").wait()\n",
    "t_env.execute_sql(FACT_ORDERS_ITEMS_KAFKA_DDL).wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "784d7ce4-2457-46f3-9082-4e7f35bbe592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.sql_query(\"SELECT * FROM fact_order_items_kafka LIMIT 1 \").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0b229f-3a04-43f3-a402-26fbbbc080fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.execute_sql(\"USE CATALOG ecomm\").wait()\n",
    "\n",
    "t_env.execute_sql(\"CREATE DATABASE IF NOT EXISTS ecomm.ecommdb\").wait()\n",
    "\n",
    "t_env.execute_sql(\"DROP TABLE IF EXISTS ecomm.ecommdb.customer_revenue_kafka\").wait()\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TABLE ecomm.ecommdb.customer_revenue_kafka (\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3),\n",
    "  PRIMARY KEY (customer_id) NOT ENFORCED\n",
    ") WITH (\n",
    "  'format-version' = '2',\n",
    "   'write.upsert.enabled' = 'true',\n",
    "  'snapshot.retention.days' = '7',\n",
    "  'write.metadata.delete-after-commit' = 'false'\n",
    ")\n",
    "\"\"\").wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb56b44c-4399-4215-b59b-12572bc49559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd22c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY VIEW v_customer_revenue_kafka AS\n",
    "SELECT\n",
    "  c.customer_id,\n",
    "  c.full_name,\n",
    "  SUM(i.quantity * i.item_price) AS total_revenue,\n",
    "  COUNT(DISTINCT o.order_id)     AS total_orders,\n",
    "  MAX(o.order_datetime)          AS last_order_ts\n",
    "FROM dim_customers_kafka AS c\n",
    "JOIN fact_orders_kafka AS o\n",
    "  ON c.customer_id = o.customer_id\n",
    "JOIN fact_order_items_kafka AS i\n",
    "  ON o.order_id = i.order_id\n",
    "WHERE o.order_status = 'PAID'\n",
    "GROUP BY c.customer_id, c.full_name\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c48c276f-c41d-4a24-9f3d-4bf575a38974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.sql_query(\"SELECT * FROM v_customer_revenue_kafka LIMIT 1\").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baee1bde-dfbc-4601-94c6-7980b3eecf7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd2620>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3)\n",
    "\"\"\"\n",
    "t_env.execute_sql(\"\"\"\n",
    "INSERT INTO ecomm.ecommdb.customer_revenue_kafka\n",
    "SELECT\n",
    "  customer_id,\n",
    "  full_name,\n",
    "  total_revenue,\n",
    "  total_orders,\n",
    "  last_order_ts\n",
    "FROM v_customer_revenue_kafka\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c20591ee-92cc-42a6-a5c3-d75259f16540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_env.sql_query(\"SELECT * FROM ecomm.ecommdb.customer_revenue_kafka\").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89d88248-cce9-4fd8-914b-17051e6c1417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd2920>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS customer_revenue_kafka_out;\").wait()\n",
    "\"\"\"\n",
    "  -- key: use customer_id as key\n",
    "  'key.fields' = 'customer_id',\n",
    "   'key.format' = 'raw',\n",
    "  -- 'key.format' = 'json',\n",
    "  \"\"\"\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE customer_revenue_kafka_out (\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3) \n",
    ") WITH (\n",
    "  'connector' = 'kafka',\n",
    "  'topic' = 'customer_revenue',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    " \n",
    "  -- value: the whole row as JSON\n",
    "  -- 'value.format' = 'json',\n",
    "  'format' = 'debezium-json',\n",
    "  'debezium-json.schema-include' = 'false',\n",
    "  'debezium-json.ignore-parse-errors' = 'true'\n",
    "  \n",
    "   -- 'value.json.fail-on-missing-field' = 'false'\n",
    "  \n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb687021-38f7-4934-8cba-c9aee3051744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf3d3d79d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"\"\"\n",
    "INSERT INTO customer_revenue_kafka_out\n",
    "SELECT\n",
    "  c.customer_id,\n",
    "  c.full_name,\n",
    "  CAST(total_revenue AS DECIMAL(18, 2)) AS total_revenue,\n",
    "  total_orders,\n",
    "  last_order_ts\n",
    "FROM v_customer_revenue_kafka AS c\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f47f2b-1917-4f46-b258-6825b6cd5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS customer_revenue_kafka_upsert\").wait()\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE customer_revenue_kafka_upsert (\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3),\n",
    "  PRIMARY KEY (customer_id) NOT ENFORCED\n",
    ") WITH (\n",
    "  'connector' = 'upsert-kafka',\n",
    "  'topic' = 'customer_revenue_upsert',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    "\n",
    "  -- key: derived from PRIMARY KEY (customer_id)\n",
    "  'key.format' = 'json',\n",
    "  'key.json.ignore-parse-errors' = 'true',\n",
    "\n",
    "  -- value: flat JSON row\n",
    "  'value.format' = 'json',\n",
    "  'value.json.fail-on-missing-field' = 'false',\n",
    "  'value.json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\").wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3123b707-dc78-4d18-b4a9-125b30d797eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd2e00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"\"\"\n",
    "INSERT INTO customer_revenue_kafka_upsert\n",
    "SELECT\n",
    "  customer_id,\n",
    "  full_name,\n",
    "  CAST(total_revenue AS DECIMAL(18, 2)) AS total_revenue,\n",
    "  total_orders,\n",
    "  last_order_ts\n",
    "FROM v_customer_revenue_kafka\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1b480a3b-c994-4398-8a89-b3ceda13f7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_env.execute_sql(\"DROP TEMPORARY TABLE IF EXISTS customer_revenue_upsert_kafka\").wait()\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TEMPORARY TABLE customer_revenue_upsert_kafka (\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3),\n",
    "  PRIMARY KEY (customer_id) NOT ENFORCED\n",
    ") WITH (\n",
    "  'connector' = 'upsert-kafka',\n",
    "  'topic' = 'customer_revenue_upsert',\n",
    "  'properties.bootstrap.servers' = 'broker:9092',\n",
    "\n",
    "  -- key = {\"customer_id\":1}\n",
    "  'key.format' = 'json',\n",
    "  'key.json.ignore-parse-errors' = 'true',\n",
    "\n",
    "  -- value = the flat JSON you showed\n",
    "  'value.format' = 'json',\n",
    "  'value.json.fail-on-missing-field' = 'false',\n",
    "  'value.json.ignore-parse-errors' = 'true'\n",
    ")\n",
    "\"\"\").wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8dc7cee5-a881-481b-9cf9-55726ed57aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional but nice for clarity\n",
    "# t_env.execute_sql(\"USE CATALOG ecomm\").wait()\n",
    "\n",
    "t_env.execute_sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS ecomm.ecommdb.customer_revenue_upsert (\n",
    "  customer_id   INT,\n",
    "  full_name     STRING,\n",
    "  total_revenue DECIMAL(18, 2),\n",
    "  total_orders  BIGINT,\n",
    "  last_order_ts TIMESTAMP(3),\n",
    "  PRIMARY KEY (customer_id) NOT ENFORCED\n",
    ") WITH (\n",
    "  'format-version' = '2',\n",
    "  'write.upsert.enabled' = 'true'\n",
    ")\n",
    "\"\"\").wait()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a81d2eeb-b85e-44b6-aea2-f73a50f436d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x7fbf38dd3d60>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_env.execute_sql(\"\"\"\n",
    "INSERT INTO ecomm.ecommdb.customer_revenue_upsert\n",
    "SELECT\n",
    "  customer_id,\n",
    "  full_name,\n",
    "  total_revenue,\n",
    "  total_orders,\n",
    "  last_order_ts\n",
    "FROM customer_revenue_upsert_kafka\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb07d9b8-4bb2-409a-b31e-ca70d6ce298f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "| op | customer_id |                      full_name |        total_revenue |         total_orders |              last_order_ts |\n",
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "| +I |           1 |                            Vel |              2026.95 |                    3 | 2025-11-01 18:45:00.000000 |\n",
      "| +I |           2 |                           Shiv |               529.49 |                    1 | 2025-10-20 11:22:00.000000 |\n",
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "2 rows in set\n"
     ]
    }
   ],
   "source": [
    "t_env.sql_query(\"SELECT * FROM ecomm.ecommdb.customer_revenue_upsert\").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96eb345b-0726-4464-8ed1-a50b932f0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "| op | customer_id |                      full_name |        total_revenue |         total_orders |              last_order_ts |\n",
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "| +I |           1 |                            Vel |              2026.95 |                    3 | 2025-11-01 18:45:00.000000 |\n",
      "| +I |           2 |                           Shiv |               529.49 |                    1 | 2025-10-20 11:22:00.000000 |\n",
      "+----+-------------+--------------------------------+----------------------+----------------------+----------------------------+\n",
      "2 rows in set\n"
     ]
    }
   ],
   "source": [
    "t_env.sql_query(\"SELECT * FROM ecomm.ecommdb.customer_revenue_kafka\").execute().print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aceef-c792-45ae-8e57-b51d0c12c2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
